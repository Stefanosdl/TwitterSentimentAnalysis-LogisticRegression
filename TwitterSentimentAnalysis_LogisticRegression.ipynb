{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TwitterSentimentAnalysis_LogisticRegression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "msvhk4OvPl9h"
      },
      "source": [
        "#link google drive\n",
        "!pip install -U -q PyDrive \n",
        "  \n",
        "from pydrive.auth import GoogleAuth \n",
        "from pydrive.drive import GoogleDrive \n",
        "from google.colab import auth \n",
        "from oauth2client.client import GoogleCredentials   \n",
        "\n",
        "# Authenticate and create the PyDrive client. \n",
        "auth.authenticate_user() \n",
        "gauth = GoogleAuth() \n",
        "gauth.credentials = GoogleCredentials.get_application_default() \n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ardxoYfZPwZ8"
      },
      "source": [
        "#get the csv file from google drive\n",
        "link = 'https://drive.google.com/file/d/1dTIWNpjlrnTQBIQtaGOh0jCRYZiAQO79/view'\n",
        "\n",
        "# to get the id part of the file \n",
        "id = link.split('/')[-2] \n",
        "\n",
        "downloaded = drive.CreateFile({'id':id})  \n",
        "downloaded.GetContentFile('SentimentTweets.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWmehci5PyPo"
      },
      "source": [
        "#read from csv and store data\n",
        "import pandas as pd \n",
        "\n",
        "df = pd.read_csv('SentimentTweets.csv',\n",
        "                encoding = 'ISO-8859-1',\n",
        "                usecols=['target', 'text'],\n",
        "                dtype='unicode')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm1MkuODP45E"
      },
      "source": [
        "df1 = df.sample(n=10000)\n",
        "features = df1['text'].values\n",
        "labels = df1['target'].values\n",
        "#features = df['text'].values\n",
        "#labels = df['target'].values"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ODaxx9dP8b2"
      },
      "source": [
        "#We use spacy for lemmatization alongside with lemminflect extension. We also use spacy for removing stopwords\n",
        "!pip3 install lemminflect\n",
        "import re\n",
        "import string\n",
        "import nltk \n",
        "import spacy\n",
        "import lemminflect\n",
        "import en_core_web_sm\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "all_stopwords = sp.Defaults.stop_words\n",
        "words = set(nltk.corpus.words.words())\n",
        "\n",
        "processed_features = []\n",
        "\n",
        "# load en_core_web_sm of English for vocabulary, syntax & entities\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "def sentence_lemmatization(my_doc, processed_feature):\n",
        "    lemma_words = []\n",
        "    for word in my_doc:\n",
        "        lemma_words.append(word._.lemma())\n",
        "    return ' '.join(lemma_words)\n",
        "\n",
        "def remove_stopwords(processed_feature):\n",
        "    text_tokens = word_tokenize(processed_feature)\n",
        "    filtered_words = [word for word in text_tokens if not word in all_stopwords]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "def removeNonEnglish(processed_feature):\n",
        "    return ' '.join(word for word in nltk.wordpunct_tokenize(processed_feature) if word.lower() in words or not word.isalpha())\n",
        "\n",
        "for sentence in features:\n",
        "    # Remove Usernames and Hashtags\n",
        "    processed_feature = ' '.join(word for word in sentence.split() if not word.startswith('@'))\n",
        "    processed_feature = ' '.join(word for word in processed_feature.split() if not word.startswith('#'))\n",
        "\n",
        "    # Remove stopwords\n",
        "    processed_feature = remove_stopwords(processed_feature)\n",
        "\n",
        "    # Remove punctuation\n",
        "    processed_feature = processed_feature.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Remove urls\n",
        "    processed_feature = re.sub(r'http\\S+|www\\S+|https\\S+', '', processed_feature)\n",
        "    \n",
        "    # Remove all the special characters and numbers\n",
        "    processed_feature = re.sub(r'\\W|\\d+', ' ', processed_feature)\n",
        "\n",
        "    # remove all single characters\n",
        "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
        "\n",
        "    # Remove single characters from the start\n",
        "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n",
        "\n",
        "    # Substituting multiple spaces with single space\n",
        "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
        "\n",
        "    # Removing prefixed 'b'\n",
        "    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
        "\n",
        "    # Remove non-Ascii characters\n",
        "    processed_feature = re.sub(r'[^\\x00-\\x7F]+',' ', processed_feature)\n",
        "\n",
        "    # Remove single space from the beginning\n",
        "    processed_feature = processed_feature.strip()\n",
        "\n",
        "    # \"nlp\" Object is used to create documents with linguistic annotations.\n",
        "    my_doc = nlp(processed_feature)\n",
        "\n",
        "    # Use spacy to lemmatize the sentences\n",
        "    processed_feature = sentence_lemmatization(my_doc, processed_feature)\n",
        "\n",
        "    processed_feature = removeNonEnglish(processed_feature)\n",
        "\n",
        "    # Remove -PROP- after lemmatization\n",
        "    processed_feature = re.sub(r'-PROP-', '', processed_feature)\n",
        "\n",
        "    # Converting to Lowercase\n",
        "    processed_feature = processed_feature.lower()\n",
        "\n",
        "    processed_features.append(processed_feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2WUlE4JP-hd"
      },
      "source": [
        "#Use TfidfVectorizer to vectorize data\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer (max_df=1.0, min_df=1, max_features=10000)\n",
        "processed_features = vectorizer.fit_transform(processed_features).toarray()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZz1mbDTQBQG"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.3, random_state=40)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-JhW_2dQC8x"
      },
      "source": [
        "#Model Development\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# default solver is incredibly slow thats why we change it\n",
        "logreg = LogisticRegression(solver = 'lbfgs')\n",
        "\n",
        "logreg.fit(X_train, y_train)\n",
        "# make predictions on the testing set\n",
        "y_pred = logreg.predict(X_test)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leQgsD3RQEyl",
        "outputId": "86ed9240-8072-4f91-a377-26bc4ef8ffd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#Evaluation\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "print(classification_report(y_test,y_pred))\n",
        "print('Accuracy score:',accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.82      0.82        17\n",
            "           4       0.81      0.81      0.81        16\n",
            "\n",
            "    accuracy                           0.82        33\n",
            "   macro avg       0.82      0.82      0.82        33\n",
            "weighted avg       0.82      0.82      0.82        33\n",
            "\n",
            "Accuracy score: 0.8181818181818182\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}